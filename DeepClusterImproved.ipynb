{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "!pip install random2\n",
    "import random2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install kneed\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "!pip install kmeans-pytorch\n",
    "from kmeans_pytorch import kmeans, kmeans_predict\n",
    "\n",
    "import gc\n",
    "\n",
    "import faiss\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "Image.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path= 'C:/Users/laura/Google Drive/WikiArt2/wikiart'\n",
    "genres = os.listdir(base_path)\n",
    "genres_f = []\n",
    "for genre in genres:\n",
    "        path = os.path.join(base_path, genre)\n",
    "        length = len(os.listdir(path))\n",
    "        if length > 1500:\n",
    "          genres_f.append(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path= 'C:/Users/laura/Google Drive/WikiArt2/wikiart'\n",
    "genres = os.listdir(base_path)\n",
    "training_data = []\n",
    "\n",
    "#print(len(training_data))\n",
    "genres_f = []\n",
    "for genre in genres:\n",
    "        path = os.path.join(base_path, genre)\n",
    "        length = len(os.listdir(path))\n",
    "        if length > 1500:\n",
    "          genres_f.append(genre)\n",
    "\n",
    "for genre in genres_f:\n",
    "        path = os.path.join(base_path, genre)\n",
    "        class_num = genres_f.index(genre) #assigning each art category a classification based on it's index in the list\n",
    "        selected = random.sample(os.listdir(path), 1000) \n",
    "        for img in selected:\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            #img_array = Image.open(os.path.join(path,img))\n",
    "            if img_array is None:\n",
    "                    print('Wrong path:', path)\n",
    "            else:\n",
    "                    img_array = Image.fromarray(img_array)\n",
    "                    aug_im = transform(img_array)\n",
    "                    #batch_image = aug_im.unsqueeze(dim=0)\n",
    "                    #sobel_im = combined(aug_im)\n",
    "                    training_data.append([aug_im, class_num])\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(training_data, 'tensor2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    training_data = torch.load('tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(training_data, batch_size=256, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeature, trainlabel = next(iter(trainloader))\n",
    "print(f\"Feature Batch Shape: {trainfeature.size()}\")\n",
    "print(f\"Label Batch Shape: {trainlabel.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple AlexNet with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path= 'C:/Users/laura/Google Drive/WikiArt2/wikiart'\n",
    "genres = os.listdir(base_path)\n",
    "training_data = []\n",
    "\n",
    "#print(len(training_data))\n",
    "genres_f = []\n",
    "for genre in genres:\n",
    "        path = os.path.join(base_path, genre)\n",
    "        length = len(os.listdir(path))\n",
    "        if length > 1500:\n",
    "          genres_f.append(genre)\n",
    "\n",
    "for genre in genres_f:\n",
    "        path = os.path.join(base_path, genre)\n",
    "        class_num = genres_f.index(genre) #assigning each art category a classification based on it's index in the list\n",
    "        selected = random.sample(os.listdir(path), 1000) \n",
    "        for img in selected:\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            #img_array = Image.open(os.path.join(path,img))\n",
    "            if img_array is None:\n",
    "                    print('Wrong path:', path)\n",
    "            else:\n",
    "                    img_array = Image.fromarray(img_array)\n",
    "                    aug_im = transform(img_array)\n",
    "                    #batch_image = aug_im.unsqueeze(dim=0)\n",
    "                    #sobel_im = combined(aug_im)\n",
    "                    training_data.append([aug_im, class_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load images with ImageFolder\n",
    "#base_path= 'C:/Users/laura/Google Drive/WikiArt2/wikiart'\n",
    "#data = datasets.ImageFolder('base_path', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=14):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 14\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = AlexNet(num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_step = len(trainloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in trainloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## whole process of AlexNet > Cluster > AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First alexnet\n",
    "\n",
    "class AlexNet1(nn.Module):\n",
    "    def __init__(self, num_classes=4096):\n",
    "        super(AlexNet1, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "num_classes = 4096\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = AlexNet1(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(trainloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_step = len(trainloader)\n",
    "num_epochs = 1\n",
    "comb_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        #labels = labels.to(device) # don't use labels\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        comb_scores.append(outputs.detach().cpu().numpy())\n",
    "        \n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "comb_scores = np.concatenate(comb_scores)\n",
    "\n",
    "print('Finished Training of AlexNet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunPCA(comb_scores):\n",
    "    mat = faiss.PCAMatrix(d_in=4096, d_out=256, eigen_power=-0.5)\n",
    "    mat.train(comb_scores)\n",
    "    x_pca = mat.apply_py(comb_scores)\n",
    "    return x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA = RunPCA(comb_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunL2(x_pca):\n",
    "    norm = np.linalg.norm(x_pca, axis=1)\n",
    "    x_l2 = x_pca / norm[:, np.newaxis]\n",
    "    x_l2 = torch.tensor(x_l2)\n",
    "    return x_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_L2 = RunL2(X_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunK_means(x_l2):\n",
    "    # set device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    # k-means\n",
    "    cluster_ids_x, cluster_centers = kmeans(\n",
    "        x_l2,\n",
    "        num_clusters= 14, \n",
    "        distance='euclidean', \n",
    "        device=device\n",
    "    )\n",
    "    return cluster_ids_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_IDS_x  = RunK_means(X_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bincount(CLUSTER_IDS_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(CLUSTER_IDS_x)\n",
    "plt.title('cluster membership counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second alexnet\n",
    "\n",
    "class AlexNet2(nn.Module):\n",
    "    def __init__(self, num_classes=14):\n",
    "        super(AlexNet2, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y= []\n",
    "\n",
    "for features, label in training_data: \n",
    "    X.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_IDS_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader([ [X[i], CLUSTER_IDS_x[i]] for i in range(len(training_data))], batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 14\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = AlexNet2(num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(trainloader)\n",
    "total_step = len(trainloader)\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    comb_scores2 = []\n",
    "    label = []\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in trainloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            comb_scores2.append(predicted.detach().cpu().numpy())\n",
    "            label.append(labels)\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) \n",
    "    comb_scores2 = np.concatenate(comb_scores2)\n",
    "    label = np.concatenate(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comb_scores2.npy', 'wb') as f:\n",
    "    np.save(f, comb_scores2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label.npy', 'wb') as f:\n",
    "    np.save(f, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comb_scores2.npy', 'rb') as f:\n",
    "    comb_scores2 = np.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label.npy', 'rb') as f:\n",
    "    label = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y.npy', 'rb') as f:\n",
    "    y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_numbers = []\n",
    "\n",
    "def create_labels():\n",
    "    for genre in genres_f:\n",
    "        class_num = genres_f.index(genre) #assigning each art category a classification based on it's index in the list\n",
    "        labels_numbers.append([genre, class_num])\n",
    "\n",
    "\n",
    "create_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(label, comb_scores2)\n",
    "\n",
    "ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=False, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_xlabel('\\nClassified Style')\n",
    "ax.set_ylabel('Pseudo label ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(labels_numbers, rotation = 90)\n",
    "ax.yaxis.set_ticklabels(labels_numbers, rotation = 360)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrt ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping labels from cluster to original labels\n",
    "def get_reference_dict(clusters,data_label):\n",
    "    reference_label = {}\n",
    "    # For loop to run through each label of cluster label\n",
    "    for i in range(len(np.unique(clusters))):\n",
    "        index = np.where(clusters == i,1,0)\n",
    "        num = np.bincount(data_label[index==1])\n",
    "        if np.size(num) != 0:\n",
    "            num = num.argmax()\n",
    "            reference_label[i] = num\n",
    "        else:\n",
    "            reference_label[i] = 6\n",
    "    return reference_label\n",
    "# Mapping predictions to original labels\n",
    "def get_labels(clusters,reference_labels):\n",
    "    temp_labels = np.random.rand(len(clusters))\n",
    "    for i in range(len(clusters)):\n",
    "        temp_labels[i] = reference_labels[clusters[i]]\n",
    "    return temp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_labels = get_reference_dict(comb_scores2, y[:13871])\n",
    "predicted_labels = get_labels(comb_scores2,reference_labels)\n",
    "print(f\"Accuracy for k = 14: \", accuracy_score(predicted_labels,y[:13871]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[:13871])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(y[:13871], predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=False, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_xlabel('\\nPredicted Style')\n",
    "ax.set_ylabel('Actual Style ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(labels_numbers, rotation = 90)\n",
    "ax.yaxis.set_ticklabels(labels_numbers, rotation = 360)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k = 64?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunPCA(comb_scoresK):\n",
    "    mat = faiss.PCAMatrix(d_in=4096, d_out=256, eigen_power=-0.5)\n",
    "    mat.train(comb_scores)\n",
    "    x_pca = mat.apply_py(comb_scoresK)\n",
    "    return x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "num_classes = 4096\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = AlexNet1(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(trainloader)\n",
    "\n",
    "total_step = len(trainloader)\n",
    "num_epochs = 1\n",
    "comb_scoresK = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        #labels = labels.to(device) # don't use labels\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        comb_scoresK.append(outputs.detach().cpu().numpy())\n",
    "        \n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "comb_scoresK = np.concatenate(comb_scoresK)\n",
    "\n",
    "print('Finished Training of AlexNet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA = RunPCA(comb_scoresK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunL2(x_pca):\n",
    "    norm = np.linalg.norm(x_pca, axis=1)\n",
    "    x_l2 = x_pca / norm[:, np.newaxis]\n",
    "    x_l2 = torch.tensor(x_l2)\n",
    "    return x_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_L2 = RunL2(X_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunK_means(x_l2):\n",
    "    # set device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    # k-means\n",
    "    cluster_ids_x, cluster_centers = kmeans(\n",
    "        x_l2,\n",
    "        num_clusters= 64, \n",
    "        distance='euclidean', \n",
    "        device=device\n",
    "    )\n",
    "    return cluster_ids_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_IDS_x  = RunK_means(X_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bincount(CLUSTER_IDS_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(CLUSTER_IDS_x)\n",
    "plt.title('cluster membership counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second alexnet\n",
    "\n",
    "class AlexNet2(nn.Module):\n",
    "    def __init__(self, num_classes=64):\n",
    "        super(AlexNet2, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y= []\n",
    "\n",
    "for features, label in training_data: \n",
    "    X.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_IDS_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader([ [X[i], CLUSTER_IDS_x[i]] for i in range(len(training_data))], batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 64\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = AlexNet2(num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(trainloader)\n",
    "total_step = len(trainloader)\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    comb_scoresK = []\n",
    "    labelK = []\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in trainloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            comb_scoresK.append(predicted.detach().cpu().numpy())\n",
    "            labelK.append(labels)\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) \n",
    "    comb_scoresK = np.concatenate(comb_scoresK)\n",
    "    labelK = np.concatenate(labelK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comb_scoresK.npy', 'wb') as f:\n",
    "    np.save(f, comb_scoresK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labelK.npy', 'wb') as f:\n",
    "    np.save(f, labelK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comb_scoresK.npy', 'rb') as f:\n",
    "    comb_scoresK = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labelK.npy', 'rb') as f:\n",
    "    labelK = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping labels from cluster to original labels\n",
    "def get_reference_dict(clusters,data_label):\n",
    "    reference_label = {}\n",
    "    # For loop to run through each label of cluster label\n",
    "    for i in range(64):\n",
    "        index = np.where(clusters == i,1,0)\n",
    "        num = np.bincount(data_label[index==1])\n",
    "        if np.size(num) != 0:\n",
    "            num = num.argmax()\n",
    "            reference_label[i] = num\n",
    "        else:\n",
    "            reference_label[i] = 6\n",
    "    return reference_label\n",
    "# Mapping predictions to original labels\n",
    "def get_labels(clusters,reference_labels):\n",
    "    temp_labels = np.random.rand(len(clusters))\n",
    "    for i in range(len(clusters)):\n",
    "        temp_labels[i] = reference_labels[clusters[i]]\n",
    "    return temp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(comb_scoresK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_labels = get_reference_dict(comb_scoresK,y[:13871])\n",
    "predicted_labels = get_labels(comb_scoresK,reference_labels)\n",
    "print(f\"Accuracy for k = 64: \", accuracy_score(predicted_labels,y[:13871]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(labelK, comb_scoresK)\n",
    "\n",
    "ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=False, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_xlabel('\\nPredicted label')\n",
    "ax.set_ylabel('Pseudo label ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "#ax.xaxis.set_ticklabels(labels_numbers, rotation = 90)\n",
    "#ax.yaxis.set_ticklabels(labels_numbers, rotation = 360)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrt ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_labels = get_reference_dict(comb_scoresK, y[:13871])\n",
    "predicted_labels = get_labels(comb_scoresK,reference_labels)\n",
    "print(f\"Accuracy for k = 64: \", accuracy_score(predicted_labels,y[:13871]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(y[:13871], predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=False, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_xlabel('\\nPredicted Style')\n",
    "ax.set_ylabel('Actual Style ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(labels_numbers, rotation = 90)\n",
    "ax.yaxis.set_ticklabels(labels_numbers, rotation = 360)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame([labelK, y]).T\n",
    "results.columns = ['cluster', 'style']\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match subclusters to ground truth styles. \n",
    "\n",
    "count style == subcluster >> \n",
    "\n",
    "New table: group per subcluster, filter max count per group, and keep subcluster and style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_results = results.groupby([\"cluster\", \"style\"]).size().reset_index(name=\"count\")\n",
    "filtered_results = counted_results.loc[counted_results.reset_index().groupby(['cluster'])['count'].idxmax()]\n",
    "cluster_to_style = filtered_results[[\"cluster\", \"style\"]].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# percentage of cluster in style\n",
    "total = pd.DataFrame(counted_results.groupby(['cluster'])['count'].sum().reset_index())\n",
    "#total\n",
    "total.columns = ['cluster', 'total']\n",
    "counted_results = counted_results.join(total, on='cluster', how = 'left', lsuffix='_left', rsuffix='_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_results['%'] = counted_results['count']/counted_results['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_results['max%'] = counted_results.groupby(['cluster_left'])['%'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permanently changes the pandas settings\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    " \n",
    "# All dataframes hereafter reflect these changes.\n",
    "display(perc_of_cluster_style)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match predictions to styles through clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(comb_scoresK)\n",
    "predictions.columns = ['prediction']\n",
    "predictions = predictions.join(cluster_to_style.set_index('cluster'), on='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['ground_truth'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predictions['style'] == predictions['ground_truth']).sum() / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_results[counted_results['%'] == counted_results['max%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_labels = get_reference_dict(cluster_to_style['cluster'], cluster_to_style['style'])\n",
    "#predicted_labels = get_labels(cluster_to_style['cluster'],reference_labels)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"Accuracy for k = 14: \", accuracy_score(cluster_to_style['cluster'],cluster_to_style['style']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
